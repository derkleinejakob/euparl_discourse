
\subsubsection{Topic Modelling with LDA}
To identify how parties talk about migration, we first have to assign speeches to a semantic topic.
For this purpose, we use Latent Dirichlet Allocation \emph{(LDA)} [TODO: source]. 
% Why LDA and not something else?
(TODO why LDA?)
% Description LDA
LDA is a probabilistic topic model. 
It assumes that in the analyzed corpus (here: the collection of speeches) there is a set of topics, which are probability distributions over all words in the corpus. 
It considers each document (here: a speech) as a bag of words that were sampled from these topics. 
For example, if a topic had high probabilities for the words "fish", "net", "water", then documents covering "fishing" would (under LDA's assumptions) have a high probability of being labelled as that topic. 
% Our LDA

We tested different parameters (numer of topics, number of iterations over the dataset) and compared 
the resulting topic coherence (TODO explanation) and the fidelity of the topics through manual inspection.
Our final model contains 30 topics (for 10 iterations) --- one of which assigns highest probabilities to the words X, Y, Z (TODO words), which we call "migration topic".

% Finding threshold
For each topic, the model assigns each speech in the corpus a probability of covering that topic. 
To find speeches covering migration, a minimum topic probability was identified manually: two authors rated a sample of 100 speeches whose migration score 
(i.e. the assigned probability of the LDA model for the migration topic of that speech) was in the range [X, Y] (TODO range) which is where we suspected the relevance threshold. 

(TODO finish this)