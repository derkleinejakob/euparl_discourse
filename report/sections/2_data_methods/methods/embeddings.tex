% I allowed myself to shorten and re-structure a bit for clarity, feel free to change back what you dont like (old version below)!
\subsubsection{Semantic Embeddings}
% TODO: move this sentence elsewhere? 
% Semantic embeddings have been widely used in political text analysis \citep{Miok2024, Nanni2021, Rudkowsky2018}. 
A pool of candidate embedding models was selected from the MTEB leaderboard \citep{enevoldsen2025mmtebmassivemultilingualtext} based on overall performance and parameter count. General-purpose semantic embeddings might primarly capture stylistic and topical variations and neglect political ideology. 
To analyze whether the embeddings include political dimensions, we test whether intra- and interparty similarity distributions differ significanlty with a two-sample Kolmogorov-Smirnov test for each candidate model. We apply Bonferroni correction accross 8 models ($\alpha = 0.05$, $\alpha\mbox{*} = 0.05 / m$). All models showed significant distributional differences, with the test statistic $\mathcal{D}$ ranging between 0.058 and 0.1.
To best capture political orientation in the embeddings, we selected the final model based on (i) intra- and interparty cosine similarities, (ii) predictive performance of a logistic regression model with political affiliation as our target variable, and (iii) K-means clustering quality measured by homogenity and completeness with respect to party membership. Based on these metrics we selected google/embeddinggemma-300m \citep{embedding_gemma_2025} to create the speech emebddings for our analyses. 


Dimensionality reduction has been used to ascertain parties ideological shift over time and to reveal underlying political dimension with word associations for each reduced axis \citep{Rheault2020-mr}. % NOTE: I added that it was PCA because to me it was not really understandable - I hope it makes sense 
Exploratory analysis using PCA showed that, although party influence is present in the principal components, it is not the defining factor of our semantic embeddings. 
To better understand how party affiliations manifest in the vector space, we identify a subspace of the embedding space in which political and ideological differences become more salient.
To this end, we employ Partial Least Squares (PLS). PLS allows us to find directions in the embedding space that are maximally associated with party labels, making it suitable for uncovering latent political dimensions that are not necessarily dominant in the overall variance of the data.

The prevalence of established migration-related rhetoric was assessed using semantic search. We used 30 migration narratives identified in a recent report by the European Commission’s Joint Research Centre \citep[p.130]{seiger_navigating_2025}, organized into four broader ``supernarratives.'' Each narrative was represented by a short descriptive sentence and embedded using the model’s built-in ``retrieval-query'' prompt. Semantic proximity between narratives and speeches was quantified using cosine similarity. Similarity scores were averaged across narratives within each supernarrative to capture high-level trends. To control for keyword-driven effects, we additionally constructed a control narrative at the presumed opposite end of the rhetorical spectrum (``We need to respect humanitarian principles in handling migration''). To validate that narrative similarity captured meaningful political differences, we correlated similarity scores with the CHES-ratings of party positions \citep{rovny_CHES_2024}. Pearson correlations were evaluated using Bonferroni-adjusted significance thresholds. Temporal trends and party-block differences in narrative prevalence were analysed using linear mixed-effects models with random intercepts and slopes at the party-block level. Because separate models were estimated for each of the five supernarratives plus the control, significance levels were Bonferroni-corrected to $p = 0.05/6$.


% old version: 
% \subsubsection{Semantic Embeddings}
% Semantic embeddings have been widely used in political text analysis \citep{Miok2024, Nanni2021, Rudkowsky2018}. Our aim is to capture patterns in how different political groups address migration. We select candidate embedding models from the MTEB leaderboard \citep{enevoldsen2025mmtebmassivemultilingualtext}, based on overall performance and parameter count. Final model selection is based on (i) intra- and interparty cosine similarities, (ii) predictive performance of a logistic regression model with political affiliation as our target variable, and (iii) Kmeans clustering quality measured by homogenity and completeness. 
% Based on these metrics we have selected google/embeddinggemma-300m \citep{embedding_gemma_2025} as our final embedding model, all analysis using semantic embeddings are conducted with this model. 

% A key concern is that general-purpose semantic embeddings may be primarly capturing stylistic and topical variations and subsequently political group ideologies influence on the embeddings might be neglegible. We test whether intra- and interparty similarity distributions differ substantially with a two-sample Kolmogorov-Smirnov test for each candidate model.
% We apply Bonferroni correction accross 8 models ($\alpha = 0.05$, $\alpha\mbox{*} = 0.05 / m$). All models showed significant distributional differences, with test statistic $\mathcal{D}$ ranging between 0.058 and 0.1.

% Dimensionality reduction has been used to ascertain parties ideological shift over time and to reveal underlying political dimension with word associations for each reduced axis  \citep{Rheault2020-mr}. Exploratory analysis showed that, although party influence is present, it is not the defining factor of our semantic embeddings. To better understand how party affiliations manifest in the vector space, we aim to identify a subspace of the embedding space in which political and ideological differences become more salient.
% To this end, we employ Partial Least Squares (PLS). PLS allows us to find directions in the embedding space that are maximally associated with party labels, making it suitable for uncovering latent political dimensions that are not necessarily dominant in the overall variance of the data.

% The prevalence of established migration-related rhetoric was assessed using semantic search in a shared embedding space. We used 30 migration narratives identified in a recent report by the European Commission’s Joint Research Centre \citep[p.130]{seiger_navigating_2025}, organized into four broader ``supernarratives.'' Each narrative was represented by a short descriptive sentence and embedded using the model’s built-in ``retrieval-query'' prompt.

% Semantic proximity between narratives and speeches was quantified using cosine similarity. Similarity scores were averaged across narratives within each supernarrative to capture high-level trends. To control for keyword-driven effects, we additionally constructed a control narrative at the presumbaly opposite end of the rhetorical spectrum (``We need to respect humanitarian principles in handling migration'').

% To validate that narrative similarity captured meaningful political differences, we correlated similarity scores with expert-coded party positions on migration policy and overall ideology \citep{rovny_CHES_2024}. Pearson correlations were evaluated using Bonferroni-adjusted significance thresholds. Temporal trends and party-block differences in narrative prevalence were analysed using linear mixed-effects models with random intercepts and slopes at the party-block level. Because separate models were estimated for each of the five supernarratives plus the control, significance levels were Bonferroni-corrected to $p = 0.05/6$.