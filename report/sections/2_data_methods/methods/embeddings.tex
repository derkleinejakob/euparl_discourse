\subsubsection{Semantic Embeddings}
Semantic embeddings have been widely used in political text analysis \citep{Miok2024, Nanni2021}. %, Rudkowsky2018}. 
A pool of candidate embedding models was selected from the MTEB leaderboard \citep{enevoldsen2025mmtebmassivemultilingualtext} based on overall performance and parameter count. 
% Note: I reordered this because it to me makes more sense to first say, all models showed significant effects; then we chose
Because general-purpose semantic embeddings may primarly capture stylistic and topical variations, we tested that ideological difference is meaningfully encoded in the embeddings. For each candidate model, we compared intra- and interparty similarity distributions using a two-sample Kolmogorov-Smirnov test with
Bonferroni correction (%$\alpha = 0.05$,
$\alpha\mbox{*} = 0.05 / m$). All models show significant distributional differences.%, with the test statistic $\mathcal{D}$ ranging between 0.058 and 0.1.

To best capture political orientation in the embeddings, we selected the final model based on (i) intra- and interparty cosine similarities, (ii) predictive performance of a logistic regression model with political affiliation as target variable, and (iii) K-means clustering quality measured by homogenity and completeness with respect to party membership. Based on this, we selected EmbeddingGemma %google/embeddinggemma-300m 
\citep{embedding_gemma_2025} to create the emebddings. 

Dimensionality reduction has been used to study ideological change over time and to recover latent political dimensions \citep{Rheault2020-mr}. % NOTE: I added that it was PCA because to me it was not really understandable - I hope it makes sense 
Exploratory PCA showed that, although party affiliations is present, it is not dominant in the overall variance of the embedding space. 
To identify a subspace in which political and ideological differences become more salient, we employ Partial Least Squares (PLS), which extracts directions maximally associated with party labels. To mitigate and quantify temporal bias, we excluded written speeches concentrated in 2015 and 2016 and % fit the PLS model on the remaining 5,433 observations, 
evaluate the model using a temporally adapted leave-one-out cross-validation strategy \citep{roberts_temporal_2017}.

The prevalence of established migration-related rhetoric is assessed using semantic search. We use 30 migration narratives identified in a recent report by the European Commission’s Joint Research Centre \citep[p.130]{seiger_navigating_2025}, organized into four broader ``supernarratives.'' Each narrative was represented by a short descriptive sentence and embedded. % using the model’s built-in ``retrieval-query'' prompt. 
Semantic proximity between narratives and speeches is quantified using cosine similarity. Similarity scores are averaged across narratives within each supernarrative to capture high-level trends. To control for keyword-driven effects, we additionally constructed a control narrative at the presumed opposite end of the semantic spectrum (``We need to respect humanitarian principles in handling migration''). To validate that narrative similarity captures meaningful political differences, we correlate similarity scores with the CHES-ratings of party positions \citep{rovny_CHES_2024}. Temporal trends and party-block differences in narrative prevalence are analysed using linear mixed-effects models with random intercepts and slopes at the party-block level. Significance levels were Bonferroni-adjusted to the number of computed Pearson correlations ($p = 0.05/246$) and linear models ($p = 0.05/6$)
