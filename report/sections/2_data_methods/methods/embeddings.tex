\subsubsection{Semantic Embeddings}
Semantic embeddings have been widely used in political text analysis \citep{Miok2024, Nanni2021, Rudkowsky2018}. 
A pool of candidate embedding models was selected from the MTEB leaderboard \citep{enevoldsen2025mmtebmassivemultilingualtext} based on overall performance and parameter count. 
% Note: I reordered this because it to me makes more sense to first say, all models showed significant effects; then we chose
Because general-purpose semantic embeddings may primarly capture stylistic and topical variations, we test whether ideological difference is meaningfully encoded in the embeddings. For each candidate model, we compare intra- and interparty similarity distributions using a two-sample Kolmogorov-Smirnov test with
Bonferroni correction ($\alpha = 0.05$, $\alpha\mbox{*} = 0.05 / m$). All models show significant distributional differences, with the test statistic $\mathcal{D}$ ranging between 0.058 and 0.1.

To best capture political orientation in the embeddings, we selected the final model based on (i) intra- and interparty cosine similarities, (ii) predictive performance of a logistic regression model with political affiliation as our target variable, and (iii) K-means clustering quality measured by homogenity and completeness with respect to party membership. Based on this, we selected google/embeddinggemma-300m \citep{embedding_gemma_2025} to create the speech emebddings for our analyses. 

Dimensionality reduction has been used to ascertain parties ideological shift over time and to reveal underlying political dimension with word associations for each reduced axis \citep{Rheault2020-mr}. % NOTE: I added that it was PCA because to me it was not really understandable - I hope it makes sense 
Exploratory analysis using PCA showed that, although party influence is present in the principal components, it is not the defining factor of our semantic embeddings. 
To better understand how party affiliations manifest in the vector space, we identify a subspace of the embedding space in which political and ideological differences become more salient.
To this end, we employ Partial Least Squares (PLS). PLS allows us to find directions in the embedding space that are maximally associated with party labels, making it suitable for uncovering latent political dimensions that are not necessarily dominant in the overall variance of the data. To mitigate and quantify temporal bias in our PLS analysis, we excluded written speeches which predominantly occured in 2015 and 2016 and fitted the PLS model on the remaining 5,433 observations. Furthermore, we employed a leave-one-out cross-validation strategy adapted for temporal data \citep{https://doi.org/10.1111/ecog.02881}.

The prevalence of established migration-related rhetoric was assessed using semantic search. We used 30 migration narratives identified in a recent report by the European Commission’s Joint Research Centre \citep[p.130]{seiger_navigating_2025}, organized into four broader ``supernarratives.'' Each narrative was represented by a short descriptive sentence and embedded using the model’s built-in ``retrieval-query'' prompt. Semantic proximity between narratives and speeches was quantified using cosine similarity. Similarity scores were averaged across narratives within each supernarrative to capture high-level trends. To control for keyword-driven effects, we additionally constructed a control narrative at the presumed opposite end of the rhetorical spectrum (``We need to respect humanitarian principles in handling migration''). To validate that narrative similarity captured meaningful political differences, we correlated similarity scores with the CHES-ratings of party positions \citep{rovny_CHES_2024}. Pearson correlations were evaluated using Bonferroni-adjusted significance thresholds. Temporal trends and party-block differences in narrative prevalence were analysed using linear mixed-effects models with random intercepts and slopes at the party-block level. Because separate models were estimated for each of the five supernarratives plus the control, significance levels were Bonferroni-corrected to $p = 0.05/6$.
